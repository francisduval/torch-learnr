---
title: "Getting started with torch"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
Sys.setenv(TORCH_INSTALL=1)
library(learnr)
library(torch)
set.seed(1)
torch_manual_seed(1)
knitr::opts_chunk$set(echo = FALSE)
```

## What is torch?

**Torch** is an R package with 2 core features:

-   Array computation with strong GPU accelation

-   Deep neural networks built on a tape-based autograd system

### Why torch?

::: {.row}
::: {.col-md-6}
-   Torch is based on PyTorch, a framework which's rapidly increasing popularity among deep learning researchers.

-   We believe others can use torch's GPU acceleration to implement fast machine learning algorithms using it's convenient interface.

-   Torch is flexible and has a low level API making it useful for a vast range of use cases, not only for deep learning.

    </div>

    <div class="col-md-6">

![Papers with code [trends](https://paperswithcode.com/trends) section.](images/Screen%20Shot%202020-11-01%20at%2017.07.12.png){width="100%"}
:::
:::

## How it's implemented?

Before going into code it's nice to understand the basic of how torch is implemented in R.

::: {.row}
::: {.col-md-6}
-   Almost all `torch_*` functions are autogenerated from LibTorch's declaration file. This means that all low level operations are automatically available in the R package.

-   Most Neural network modules, optimizers and datasets and dataloaders code is writen in R. It's easy to inspect the code and use it to write your own customizations.

-   The diagram shows LibTorch in the bottom and all the layers implemented to allow using torch from R.

-   Differently from TensorFlow for R that is built on top of the Python implementation via reticulate, torch is built on top of LibTorch, the C++ API of PyTorch and thus, `torch` has no Python dependency.

    </div>

    <div class="col-md-6">

![Implementation diagram.](images/implementation.png){width="100%"}
:::
:::

## Torch components

The torch package has a few core components that we will discuss in this tutorial.

::: {.row}
::: {.col-md-6}
![Components of the torch package.](images/components.png){width="100%"}
:::
:::


Some of the components can be classified as low-level like:

-   **Tensors**: a multi-dimensional array container. Tensors are used to represent data and parameters in torch.

-   **Array computation library**: Torch includes more than 200 functions that can be used to transform tensors, including mathematical operations, structure transformations, etc.

-   **Autograd**: allows computing exact derivatives of tensor operations, this is essential to allow torch to be used for machine learning.

Torch also provides higher level abstractions that make it easier to build larger programs:

-   **Neural network modules**: `nn_modules` are a higher level abstraction to make it easy to handle state when composing many tensor operations.

-   **Datasets**: an abstraction for loading data in torch. It allows handling data that you might not be able to load entirely into a single tensor because of memory limitations.

-   **Dataloaders**: simplifies loading a dataset into batches and allows distributing data-loading into multiple processes.

## Creating tensors {data-progressive=TRUE}

Tensors are the core data structure in torch.

### From R objects

Tensors can be created from R atomic vectors, matrices and arrays suing the `torch_tensor` function:

```{r atomic, exercise=TRUE}
x <- torch_tensor(c(1,2,3))
x
```

```{r matrix, exercise=TRUE}
m <- matrix(runif(6), nrow = 2)
x <- torch_tensor(m)
x
```

```{r array, exercise=TRUE}
a <- array(runif(12), dim = c(2, 3, 2))
x <- torch_tensor(a)
x
```

### Using initialization functions

Tensors can also be created using torch initialization functions.
The first argument for most initialization functions is a sequence of integers indicating the shape of the tensor.

```{r randn, exercise=TRUE}
x <- torch_randn(2,2)
x
```

Other functions to create tensors are:

- `torch_arange`: Returns a tensor with a sequence of integers,
- `torch_full`: Returns a tensor filled with a single value,
- `torch_ones`: Returns a tensor filled with all ones,
- `torch_rand`: Returns a tensor filled with values drawn from a uniform distribution on [0, 1).
- `torch_randn`: Returns a tensor filled with values drawn from a unit normal distribution,
- `torch_zeros`: Returns a tensor filled with all zeros.

You can see the full list [here](https://torch.mlverse.org/docs/articles/tensor-creation.html#using-creation-functions-1)

Feel free to experiment with other initialization function below:

```{r experiment, exercise=TRUE}



```

## Tensor attributes {data-progressive=TRUE}

Every tensor has a few attributes that determine how data is stored in the computer. The most important attribute are described below:

<!-- <div class = "row"> -->
<!-- <div class="col-md-6"></div> -->
<!-- <div class="col-md-6"></div> -->
<!-- </div> -->

<div class = "row">
<div class="col-md-6">
- **data type**: describes how the bytes in the fixed-size block of memory corresponding to an array item should be interpreted. For example: int, float, double, etc.
- **device**: describes in which device the memory corresponding to that tensor lives. For example: a **cuda** device or **cpu**. Operations on tensors happens on the device it's located.
- **dimensions**: describes the shape of the tensor.
- **requires_grad**: a flag indicating if operations on that tensor should be recorded to allow autograd in the future.
</div>
<div class="col-md-6">
![Tensor attributes.](images/tensor.png){width="100%"}
</div>
</div>

### Accessing attributes

Tensor's attributes can be accessed using the `$` operator:

```{r dtype, exercise=TRUE}
x <- torch_randn(2,2)
x$dtype
```

```{r device, exercise=TRUE, exercise.setup="dtype"}
x$device
```

```{r shape, exercise=TRUE, exercise.setup="dtype"}
x$shape
```

```{r req_grad, exercise=TRUE, exercise.setup="dtype"}
x$requires_grad
```

### Modifying attributes

The default tensor attributes can be modified when creating the tensors or using the `$to` method. 

```{r cre, exercise=TRUE}
x <- torch_randn(2,2, dtype = torch_float64())
x
```

```{r to, exercise=TRUE}
x <- torch_randn(2,2)
x <- x$to(dtype = torch_float16())
x
```

The `$to` method creates a copy of that tensor with modified attributes.

### CUDA devices

Moving between devices is also done with the `$to` method, but devices only `cpu` devices are available to all systems. CUDA devices are only available if your system is equipped with a compatible GPU.
You can check if they are available with:

```{r cuda, exercise=TRUE}
cuda_is_available()
```

A common pattern in torch is to create a device object at the beginning of your script and reuse it as you create and move tensors. For example:

```{r cuda2, exercise=TRUE}
device <- if (cuda_is_available()) "cuda" else "cpu"

x <- torch_randn(2, 2, device = device)
y <- x$to(device = device)
```

Moving between devices is an important operation because tensor operations happens in the device the
tensor is located, so if you want to use the fast GPU implementations, you need to move tensors to the CUDA device.

### Exercise

Create a tensor with the `torch_bool()` data type and cast it to `torch_double()` type using the `$to()` method.

```{r booldouble, exercise=TRUE, exercise.eval=FALSE}



```

## Indexing tensors {data-progressive=TRUE}

Indexing tensors in torch is very similar to indexing vectors, matrices and arrays in R with an important difference when using **negative** indexes.

In torch negative indexes don't remove the element,
instead it selects starting from the end. We decided for this behavior because it seems that selecting starting from the end is more frequently used than removing elements in torch applications.

Considering a tensor with:

```{r index1, exercise=TRUE}
x <- torch_tensor(1:5)
x
```

```{r index2, exercise=TRUE, exercise.setup="index1"}
# takes the first element
x[1]
```

```{r index3, exercise=TRUE, exercise.setup="index1"}
# negative index: take the first element starting from end, ie. last element
x[-1]
```

### Selecting intervals

Torch also supports indexing tensors by intervals by boolean and integer tensors and by R vectors.
See the examples below:

```{r index4, exercise=TRUE}
x <- torch_tensor(1:5)
x
```

```{r index5, exercise=TRUE, exercise.setup="index4"}
# selecting the first 3 elements
x[1:3]
```

```{r index6, exercise=TRUE, exercise.setup="index4"}
# selecting the from the third element until the end. the `N` syntax is syntax sugar for the last index
x[3:N]
```

```{r index7, exercise=TRUE, exercise.setup="index4"}
# selecting the last 2 elements
x[-2:N]
```

```{r index8, exercise=TRUE, exercise.setup="index4"}
# selecting using a boolean tensor
x[x>2]
```

### Multi-dimensional selections

When indexing a tensor with multiple dimensions, you can use a different indexes separating by commas, just like in R. For example:

```{r index9, exercise=TRUE}
x <- torch_randn(2, 2, 3)
dim(x)
```

```{r index10, exercise=TRUE, exercise.setup = "index9"}
# selecting the first element in every dimension
x[1, 1, 1]
```

```{r index11, exercise=TRUE, exercise.setup = "index9"}
# you can select everything from a dimension by using an empty argument.
x[,,1]
```

```{r index12, exercise=TRUE, exercise.setup = "index9"}
# you can use .. when you want to take all elements from all dimensions until the last.
x[..,1]
```

```{r index13, exercise=TRUE, exercise.setup = "index9"}
# you can also add a new dimension using the `newaxis` sugar
x[..,newaxis]
```

```{r index14, exercise=TRUE, exercise.setup = "index9"}
# by default when you select a single element from a dimension, it's dropped. 
# you can change this behavior by setting drop = FALSE
x[1,..,drop=FALSE]
```

```{r index15, exercise=TRUE, exercise.setup = "index9"}
# subset assignment is also supported
x[1,1,1] <- 0
x[1,1,1]
```





